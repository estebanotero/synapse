{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformo un CSV a archivo Parquet"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%pyspark # configuro el lenguaje que voy a utilizar en el bloque\r\n",
        "\r\n",
        "# Armo un dataframe con los dato del CSV (si tiene headers lo tengo que especificar , header=True)\r\n",
        "df = spark.read.load('abfss://gomc@storageaccdesarrollo.dfs.core.windows.net/TC_1812.csv', format='csv')\r\n",
        "\r\n",
        "# Muestro las primeras 10 lineas del dataframe\r\n",
        "display(df.limit(10))"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "# Nuevos nombres de columnas\r\n",
        "lista_titulos = [\"Fecha\", \"ARS\", \"BRL\", \"EUR\", \"CLP\", \"PYG\", \"UYU\",\"Base\"]\r\n",
        "\r\n",
        "# Asignar nuevos nombres a las columnas\r\n",
        "for i, nombre_columans in enumerate(lista_titulos):\r\n",
        "    df = df.withColumnRenamed(df.columns[i], nombre_columans)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Muestro el dataframe con los nuevos nombres\r\n",
        "df.show()"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "# Tomo el dataframe y lo convierto en un archivo Parquet\r\n",
        "df.write.option(\"header\",True).parquet('abfss://gomc@storageaccdesarrollo.dfs.core.windows.net/tipocambio')"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}